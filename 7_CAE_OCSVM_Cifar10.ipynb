{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUGw6b2-LJre"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "from keras.datasets import mnist, fashion_mnist, cifar100, cifar10\n",
        "from keras.backend import cast_to_floatx\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0MHJ4pVLoec"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "random.seed(0)\n",
        "weight_decay = 0.000005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi93ip8mLuBm"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import itertools\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import apply_affine_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqege8c9MAiX"
      },
      "source": [
        "## **Load and Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXT02KNLzm6"
      },
      "outputs": [],
      "source": [
        "def resize_and_crop_image(input_file, output_side_length, greyscale=False):\n",
        "    img = cv2.imread(input_file)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB if not greyscale else cv2.COLOR_BGR2GRAY)\n",
        "    height, width = img.shape[:2]\n",
        "    new_height = output_side_length\n",
        "    new_width = output_side_length\n",
        "    if height > width:\n",
        "        new_height = int(output_side_length * height / width)\n",
        "    else:\n",
        "        new_width = int(output_side_length * width / height)\n",
        "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "    height_offset = (new_height - output_side_length) // 2\n",
        "    width_offset = (new_width - output_side_length) // 2\n",
        "    cropped_img = resized_img[height_offset:height_offset + output_side_length,\n",
        "                              width_offset:width_offset + output_side_length]\n",
        "    assert cropped_img.shape[:2] == (output_side_length, output_side_length)\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoWPCnk3MOa1"
      },
      "outputs": [],
      "source": [
        "def normalize_minus1_1(data):\n",
        "    return 2*(data/255.) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHgParnmMRw7"
      },
      "outputs": [],
      "source": [
        "def get_channels_axis():\n",
        "  import keras\n",
        "  idf = keras.backend.image_data_format()\n",
        "  if idf == 'channels_first':\n",
        "      return 1\n",
        "  assert idf == 'channels_last'\n",
        "  return 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVGJ2jWd6-9Z"
      },
      "outputs": [],
      "source": [
        "def load_cifar10():\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train = normalize_minus1_1(cast_to_floatx(X_train))\n",
        "    X_test = normalize_minus1_1(cast_to_floatx(X_test))\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZL18EvHMl8b"
      },
      "source": [
        "## **Saving the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7NtHmLmMfoN"
      },
      "outputs": [],
      "source": [
        "def save_roc_pr_curve_data(scores, labels):\n",
        "    scores = scores.flatten()\n",
        "    labels = labels.flatten()\n",
        "\n",
        "    scores_pos = scores[labels == 1]\n",
        "    scores_neg = scores[labels != 1]\n",
        "\n",
        "    truth = np.concatenate((np.zeros_like(scores_neg), np.ones_like(scores_pos)))\n",
        "    preds = np.concatenate((scores_neg, scores_pos))\n",
        "    fpr, tpr, roc_thresholds = roc_curve(truth, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc\",roc_auc)\n",
        "\n",
        "    # pr curve where \"normal\" is the positive class\n",
        "    precision_norm, recall_norm, pr_thresholds_norm = precision_recall_curve(truth, preds)\n",
        "    pr_auc_norm = auc(recall_norm, precision_norm)\n",
        "    print(\"pr_auc_norm where normal is the positive class\",pr_auc_norm)\n",
        "\n",
        "    # pr curve where \"anomaly\" is the positive class\n",
        "    precision_anom, recall_anom, pr_thresholds_anom = precision_recall_curve(truth, -preds, pos_label=0)\n",
        "    pr_auc_anom = auc(recall_anom, precision_anom)\n",
        "    print(\"pr_auc_norm where anomaly is the positive class\",pr_auc_anom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEEuib19NZ26"
      },
      "outputs": [],
      "source": [
        "def get_class_name_from_index(index, dataset_name):\n",
        "    ind_to_name = {\n",
        "        'cifar10': ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n",
        "    }\n",
        "\n",
        "    return ind_to_name[dataset_name][index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA18t2j0Mq9Z"
      },
      "source": [
        "## **Transformations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOM4GuZqMqDS"
      },
      "outputs": [],
      "source": [
        "class AffineTransformation(object):\n",
        "    def __init__(self, flip, tx, ty, k_90_rotate):\n",
        "        self.flip = flip\n",
        "        self.tx = tx\n",
        "        self.ty = ty\n",
        "        self.k_90_rotate = k_90_rotate\n",
        "\n",
        "    def __call__(self, x):\n",
        "        res_x = x\n",
        "        if self.flip:\n",
        "            res_x = np.fliplr(res_x)\n",
        "        if self.tx != 0 or self.ty != 0:\n",
        "            res_x = apply_affine_transform(res_x, tx=self.tx, ty=self.ty,row_axis=0,\n",
        "    col_axis=1, channel_axis=2, fill_mode='reflect')\n",
        "        if self.k_90_rotate != 0:\n",
        "            res_x = np.rot90(res_x, self.k_90_rotate)\n",
        "\n",
        "        return res_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QRX0FkLMybA"
      },
      "outputs": [],
      "source": [
        "class AbstractTransformer(abc.ABC):\n",
        "    def __init__(self):\n",
        "        self._transformation_list = None\n",
        "        self._create_transformation_list()\n",
        "\n",
        "    @property\n",
        "    def n_transforms(self):\n",
        "        return len(self._transformation_list)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _create_transformation_list(self):\n",
        "        return\n",
        "\n",
        "    def transform_batch(self, x_batch, t_inds):\n",
        "        assert len(x_batch) == len(t_inds)\n",
        "\n",
        "        transformed_batch = x_batch.copy()\n",
        "        for i, t_ind in enumerate(t_inds):\n",
        "            transformed_batch[i] = self._transformation_list[t_ind](transformed_batch[i])\n",
        "        return transformed_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "totupBB0M1PE"
      },
      "outputs": [],
      "source": [
        "class Transformer(AbstractTransformer):\n",
        "    def __init__(self, translation_x=8, translation_y=8):\n",
        "        self.max_tx = translation_x\n",
        "        self.max_ty = translation_y\n",
        "        super().__init__()\n",
        "\n",
        "    def _create_transformation_list(self):\n",
        "        transformation_list = []\n",
        "        for is_flip, tx, ty, k_rotate in itertools.product((False, True),\n",
        "                                                           (0, -self.max_tx, self.max_tx),\n",
        "                                                           (0, -self.max_ty, self.max_ty),\n",
        "                                                           range(4)):\n",
        "            transformation = AffineTransformation(is_flip, tx, ty, k_rotate)\n",
        "            transformation_list.append(transformation)\n",
        "\n",
        "        self._transformation_list = transformation_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFHtMXniM4nl"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformer(AbstractTransformer):\n",
        "    def _create_transformation_list(self):\n",
        "        transformation_list = []\n",
        "        for is_flip, k_rotate in itertools.product((False, True),\n",
        "                                                    range(4)):\n",
        "            transformation = AffineTransformation(is_flip, 0, 0, k_rotate)\n",
        "            transformation_list.append(transformation)\n",
        "\n",
        "        self._transformation_list = transformation_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5wCGPjM9zT"
      },
      "source": [
        "## **The Model: Wide Residual Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdBSbryzJLl3"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Activation, BatchNormalization, Flatten, Dense, Conv2DTranspose, Reshape\n",
        "\n",
        "\n",
        "def conv_encoder(input_side=32, n_channels=3, representation_dim=256, representation_activation='tanh',\n",
        "                 intermediate_activation='relu'):\n",
        "    nf = 64\n",
        "    input_shape = (n_channels, input_side, input_side) if get_channels_axis() == 1 else (input_side, input_side,\n",
        "                                                                                         n_channels)\n",
        "\n",
        "    x_in = Input(shape=input_shape)\n",
        "    enc = x_in\n",
        "\n",
        "    # downsample x0.5\n",
        "    enc = Conv2D(nf, kernel_size=(3, 3), strides=(2, 2), padding='same')(enc)\n",
        "    enc = BatchNormalization(axis=get_channels_axis())(enc)\n",
        "    enc = Activation(intermediate_activation)(enc)\n",
        "\n",
        "    # downsample x0.5\n",
        "    enc = Conv2D(nf * 2, kernel_size=(3, 3), strides=(2, 2), padding='same')(enc)\n",
        "    enc = BatchNormalization(axis=get_channels_axis())(enc)\n",
        "    enc = Activation(intermediate_activation)(enc)\n",
        "\n",
        "    # downsample x0.5\n",
        "    enc = Conv2D(nf * 4, kernel_size=(3, 3), strides=(2, 2), padding='same')(enc)\n",
        "    enc = BatchNormalization(axis=get_channels_axis())(enc)\n",
        "    enc = Activation(intermediate_activation)(enc)\n",
        "\n",
        "    if input_side == 64:\n",
        "        # downsample x0.5\n",
        "        enc = Conv2D(nf * 8, kernel_size=(3, 3), strides=(2, 2), padding='same')(enc)\n",
        "        enc = BatchNormalization(axis=get_channels_axis())(enc)\n",
        "        enc = Activation(intermediate_activation)(enc)\n",
        "\n",
        "    enc = Flatten()(enc)\n",
        "    rep = Dense(representation_dim, activation=representation_activation)(enc)\n",
        "\n",
        "    return Model(x_in, rep)\n",
        "\n",
        "\n",
        "def conv_decoder(output_side=32, n_channels=3, representation_dim=256, activation='relu'):\n",
        "    nf = 64\n",
        "\n",
        "    rep_in = Input(shape=(representation_dim,))\n",
        "\n",
        "    g = Dense(nf * 4 * 4 * 4)(rep_in)\n",
        "    g = BatchNormalization(axis=-1)(g)\n",
        "    g = Activation(activation)(g)\n",
        "\n",
        "    conv_shape = (nf * 4, 4, 4) if get_channels_axis() == 1 else (4, 4, nf * 4)\n",
        "    g = Reshape(conv_shape)(g)\n",
        "\n",
        "    # upsample x2\n",
        "    g = Conv2DTranspose(nf * 2, kernel_size=(3, 3), strides=(2, 2), padding='same')(g)\n",
        "    g = BatchNormalization(axis=get_channels_axis())(g)\n",
        "    g = Activation(activation)(g)\n",
        "\n",
        "    # upsample x2\n",
        "    g = Conv2DTranspose(nf, kernel_size=(3, 3), strides=(2, 2), padding='same')(g)\n",
        "    g = BatchNormalization(axis=get_channels_axis())(g)\n",
        "    g = Activation(activation)(g)\n",
        "\n",
        "    if output_side == 64:\n",
        "        # upsample x2\n",
        "        g = Conv2DTranspose(nf, kernel_size=(3, 3), strides=(2, 2), padding='same')(g)\n",
        "        g = BatchNormalization(axis=get_channels_axis())(g)\n",
        "        g = Activation(activation)(g)\n",
        "\n",
        "    # upsample x2\n",
        "    g = Conv2DTranspose(n_channels, kernel_size=(3, 3), strides=(2, 2), padding='same')(g)\n",
        "    g = Activation('tanh')(g)\n",
        "\n",
        "    return Model(rep_in, g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2kvANONx2t"
      },
      "source": [
        "## **Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpVXqZTTN-Hi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "from multiprocessing import Manager, freeze_support, Process\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from scipy.special import psi, polygamma\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from joblib import Parallel, delayed\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIOm47AHOIOj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "from keras.datasets import mnist, fashion_mnist, cifar100, cifar10\n",
        "from keras.backend import cast_to_floatx\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSJ4PYiINJwB"
      },
      "outputs": [],
      "source": [
        "def _train_ocsvm_and_score(params, xtrain, test_labels, xtest):\n",
        "    return roc_auc_score(test_labels, OneClassSVM(**params).fit(xtrain).decision_function(xtest))\n",
        "def _cae_ocsvm_experiment(dataset_load_fn, dataset_name, single_class_ind, gpu_q):\n",
        "    print('Class:', single_class_ind)\n",
        "    gpu_to_use = gpu_q.get()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_to_use\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = dataset_load_fn()\n",
        "    x_train = x_train[:TRAIN_SIZE]\n",
        "    y_train =  y_train[:TRAIN_SIZE]\n",
        "    x_test =  x_test[:VAL_SIZE]\n",
        "    y_test =  y_test[:VAL_SIZE]\n",
        "\n",
        "    n_channels = x_train.shape[get_channels_axis()]\n",
        "    input_side = x_train.shape[2]  # channel side will always be at shape[2]\n",
        "    enc = conv_encoder(input_side, n_channels)\n",
        "    dec = conv_decoder(input_side, n_channels)\n",
        "    x_in = Input(shape=x_train.shape[1:])\n",
        "    x_rec = dec(enc(x_in))\n",
        "    cae = Model(x_in, x_rec)\n",
        "    cae.compile('adam', 'mse')\n",
        "\n",
        "    x_train_task = x_train[y_train.flatten() == single_class_ind]\n",
        "    x_test_task = x_test[y_test.flatten() == single_class_ind]  # This is just for visual monitoring\n",
        "    cae.fit(x=x_train_task, y=x_train_task, batch_size=128, epochs=10, validation_data=(x_test_task, x_test_task))\n",
        "\n",
        "    x_train_task_rep = enc.predict(x_train_task, batch_size=128)\n",
        "\n",
        "    x_test_rep = enc.predict(x_test, batch_size=128)\n",
        "    pg = ParameterGrid({'nu': np.linspace(0.1, 0.9, num=9),\n",
        "                        'gamma': np.logspace(-7, 2, num=10, base=2)})\n",
        "\n",
        "    results = Parallel(n_jobs=6)(\n",
        "        delayed(_train_ocsvm_and_score)(d, x_train_task_rep, y_test.flatten() == single_class_ind, x_test_rep)\n",
        "        for d in pg)\n",
        "\n",
        "    best_params, best_auc_score = max(zip(pg, results), key=lambda t: t[-1])\n",
        "    best_ocsvm = OneClassSVM(**best_params).fit(x_train_task_rep)\n",
        "    scores = best_ocsvm.decision_function(x_test_rep)\n",
        "    labels = y_test.flatten() == single_class_ind\n",
        "\n",
        "    save_roc_pr_curve_data(scores, labels)\n",
        "\n",
        "    gpu_q.put(gpu_to_use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzJ1wKT7OatO"
      },
      "outputs": [],
      "source": [
        "def run_experiments(load_dataset_fn, dataset_name, q, n_classes):\n",
        "    print(\"START _transformations_experiment \")\n",
        "    # CAE OC-SVM\n",
        "    n_runs = 3\n",
        "    for i in range(n_runs):\n",
        "        print('Run Number:',i+1)\n",
        "        processes = [Process(target=_cae_ocsvm_experiment,\n",
        "                         args=(load_dataset_fn, dataset_name, c, q)) for c in range(n_classes)]\n",
        "        for p in processes:\n",
        "            p.start()\n",
        "            p.join()\n",
        "    print(\"END _transformations_experiment \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqhuKeYOrEm"
      },
      "outputs": [],
      "source": [
        "RESULTS_DIR = ''\n",
        "TRAIN_SIZE = 10000\n",
        "VAL_SIZE = 1000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_3zJdh_O0XD",
        "outputId": "57634f68-c002-4098-980f-b2f533d40c95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START _transformations_experiment \n",
            "Run Number: 1\n",
            "Class: 0\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.391 - ETA: 1s - loss: 0.360 - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.259 - 3s 239ms/step - loss: 0.2597 - val_loss: 0.2727\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.129 - 1s 142ms/step - loss: 0.1295 - val_loss: 0.2367\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - 1s 161ms/step - loss: 0.0965 - val_loss: 0.2077\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.082 - 2s 201ms/step - loss: 0.0823 - val_loss: 0.2000\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 187ms/step - loss: 0.0724 - val_loss: 0.1976\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.066 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - 2s 196ms/step - loss: 0.0662 - val_loss: 0.2005\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.058 - ETA: 1s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.061 - 2s 196ms/step - loss: 0.0612 - val_loss: 0.1978\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.054 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 2s 208ms/step - loss: 0.0576 - val_loss: 0.2044\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.052 - ETA: 1s - loss: 0.054 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - 2s 192ms/step - loss: 0.0555 - val_loss: 0.2048\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.051 - ETA: 1s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - 1s 172ms/step - loss: 0.0523 - val_loss: 0.2046\n",
            "roc_auc 0.5614237317487634\n",
            "pr_auc_norm where normal is the positive class 0.1328237625068519\n",
            "pr_auc_norm where anomaly is the positive class 0.9145807468370819\n",
            "Class: 1\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.44 - ETA: 0s - loss: 0.4096 - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.296 - 3s 202ms/step - loss: 0.2961 - val_loss: 0.2775\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.180 - ETA: 1s - loss: 0.178 - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.158 - 1s 187ms/step - loss: 0.1580 - val_loss: 0.2432\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.125 - 1s 160ms/step - loss: 0.1255 - val_loss: 0.2185\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.109 - 1s 180ms/step - loss: 0.1098 - val_loss: 0.2074\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.102 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.099 - 1s 174ms/step - loss: 0.0993 - val_loss: 0.2052\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.097 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - 1s 171ms/step - loss: 0.0912 - val_loss: 0.2064\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - 1s 171ms/step - loss: 0.0851 - val_loss: 0.2096\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 178ms/step - loss: 0.0788 - val_loss: 0.2114\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.075 - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - 1s 181ms/step - loss: 0.0740 - val_loss: 0.2125\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 1s 184ms/step - loss: 0.0704 - val_loss: 0.2135\n",
            "roc_auc 0.5211337090985335\n",
            "pr_auc_norm where normal is the positive class 0.09740908600018816\n",
            "pr_auc_norm where anomaly is the positive class 0.9286145936914427\n",
            "Class: 2\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 12s - loss: 0.36 - ETA: 1s - loss: 0.3442 - ETA: 1s - loss: 0.314 - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.226 - 3s 211ms/step - loss: 0.2269 - val_loss: 0.1963\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.131 - ETA: 1s - loss: 0.130 - ETA: 1s - loss: 0.127 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.118 - 1s 165ms/step - loss: 0.1183 - val_loss: 0.1767\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.113 - ETA: 1s - loss: 0.102 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - 1s 147ms/step - loss: 0.0935 - val_loss: 0.1666\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.081 - ETA: 1s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - 1s 168ms/step - loss: 0.0816 - val_loss: 0.1651\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.080 - ETA: 1s - loss: 0.079 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 163ms/step - loss: 0.0729 - val_loss: 0.1597\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 1s 157ms/step - loss: 0.0674 - val_loss: 0.1613\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.064 - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.062 - 2s 170ms/step - loss: 0.0624 - val_loss: 0.1624\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.064 - ETA: 1s - loss: 0.064 - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - 1s 151ms/step - loss: 0.0598 - val_loss: 0.1562\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.056 - 1s 157ms/step - loss: 0.0573 - val_loss: 0.1543\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.059 - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 1s 152ms/step - loss: 0.0571 - val_loss: 0.1573\n",
            "roc_auc 0.6609111111111112\n",
            "pr_auc_norm where normal is the positive class 0.1982081600412617\n",
            "pr_auc_norm where anomaly is the positive class 0.9354665157549075\n",
            "Class: 3\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 11s - loss: 0.43 - ETA: 1s - loss: 0.4028 - ETA: 1s - loss: 0.366 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.274 - 3s 229ms/step - loss: 0.2744 - val_loss: 0.2428\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.145 - 1s 163ms/step - loss: 0.1454 - val_loss: 0.2054\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.123 - ETA: 1s - loss: 0.121 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.109 - 1s 172ms/step - loss: 0.1099 - val_loss: 0.1841\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.098 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - 1s 188ms/step - loss: 0.0937 - val_loss: 0.1804\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - 1s 179ms/step - loss: 0.0847 - val_loss: 0.1774\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 178ms/step - loss: 0.0770 - val_loss: 0.1797\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 1s 149ms/step - loss: 0.0720 - val_loss: 0.1829\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - 1s 159ms/step - loss: 0.0664 - val_loss: 0.1841\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - 1s 154ms/step - loss: 0.0619 - val_loss: 0.1811\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 1s 179ms/step - loss: 0.0578 - val_loss: 0.1786\n",
            "roc_auc 0.5150934614843438\n",
            "pr_auc_norm where normal is the positive class 0.10280939060561302\n",
            "pr_auc_norm where anomaly is the positive class 0.9186325024354849\n",
            "Class: 4\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.32 - ETA: 0s - loss: 0.3034 - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.243 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.212 - 3s 216ms/step - loss: 0.2124 - val_loss: 0.2032\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.131 - ETA: 1s - loss: 0.132 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.112 - 2s 194ms/step - loss: 0.1125 - val_loss: 0.1790\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - 1s 164ms/step - loss: 0.0841 - val_loss: 0.1676\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 189ms/step - loss: 0.0715 - val_loss: 0.1614\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - 1s 160ms/step - loss: 0.0628 - val_loss: 0.1610\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 1s 169ms/step - loss: 0.0573 - val_loss: 0.1602\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - 1s 175ms/step - loss: 0.0529 - val_loss: 0.1586\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.048 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - 2s 206ms/step - loss: 0.0501 - val_loss: 0.1601\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.046 - ETA: 1s - loss: 0.046 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.046 - 1s 157ms/step - loss: 0.0469 - val_loss: 0.1628\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.044 - 1s 154ms/step - loss: 0.0445 - val_loss: 0.1572\n",
            "roc_auc 0.684017094017094\n",
            "pr_auc_norm where normal is the positive class 0.18440679360168707\n",
            "pr_auc_norm where anomaly is the positive class 0.9488960826903461\n",
            "Class: 5\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.399 - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.278 - 3s 230ms/step - loss: 0.2780 - val_loss: 0.2465\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.169 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.147 - 1s 161ms/step - loss: 0.1478 - val_loss: 0.2186\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 0.123 - ETA: 1s - loss: 0.119 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - 1s 157ms/step - loss: 0.1119 - val_loss: 0.1985\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.099 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - 1s 167ms/step - loss: 0.0962 - val_loss: 0.1837\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - 2s 198ms/step - loss: 0.0869 - val_loss: 0.1824\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.081 - ETA: 1s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 188ms/step - loss: 0.0799 - val_loss: 0.1823\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.073 - ETA: 1s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - 1s 185ms/step - loss: 0.0743 - val_loss: 0.1844\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.068 - ETA: 1s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 1s 193ms/step - loss: 0.0693 - val_loss: 0.1853\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - 1s 190ms/step - loss: 0.0655 - val_loss: 0.1884\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - 1s 170ms/step - loss: 0.0612 - val_loss: 0.1858\n",
            "roc_auc 0.4815149356266857\n",
            "pr_auc_norm where normal is the positive class 0.07812601440805872\n",
            "pr_auc_norm where anomaly is the positive class 0.9169420797189317\n",
            "Class: 6\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 11s - loss: 0.38 - ETA: 1s - loss: 0.3495 - ETA: 1s - loss: 0.325 - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.234 - 3s 223ms/step - loss: 0.2349 - val_loss: 0.2076\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.152 - ETA: 1s - loss: 0.142 - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.120 - 1s 169ms/step - loss: 0.1209 - val_loss: 0.1697\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.097 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 1s 167ms/step - loss: 0.0951 - val_loss: 0.1584\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.084 - ETA: 1s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - 2s 167ms/step - loss: 0.0842 - val_loss: 0.1577\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.080 - ETA: 1s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 162ms/step - loss: 0.0782 - val_loss: 0.1609\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.073 - ETA: 1s - loss: 0.074 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - 2s 175ms/step - loss: 0.0741 - val_loss: 0.1571\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.065 - ETA: 1s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 1s 161ms/step - loss: 0.0689 - val_loss: 0.1509\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.067 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - 1s 164ms/step - loss: 0.0669 - val_loss: 0.1556\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.062 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 2s 184ms/step - loss: 0.0645 - val_loss: 0.1543\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.061 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - 1s 159ms/step - loss: 0.0621 - val_loss: 0.1511\n",
            "roc_auc 0.7957991473616474\n",
            "pr_auc_norm where normal is the positive class 0.32395594220281787\n",
            "pr_auc_norm where anomaly is the positive class 0.9674001955548455\n",
            "Class: 7\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.41 - ETA: 1s - loss: 0.3966 - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.279 - 3s 226ms/step - loss: 0.2793 - val_loss: 0.2375\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.175 - ETA: 1s - loss: 0.168 - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.150 - 1s 182ms/step - loss: 0.1509 - val_loss: 0.2112\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.122 - ETA: 1s - loss: 0.121 - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - 1s 146ms/step - loss: 0.1150 - val_loss: 0.1958\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - 1s 152ms/step - loss: 0.0987 - val_loss: 0.1912\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.088 - 1s 193ms/step - loss: 0.0885 - val_loss: 0.1928\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - 1s 162ms/step - loss: 0.0802 - val_loss: 0.1918\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - 1s 163ms/step - loss: 0.0754 - val_loss: 0.1956\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 176ms/step - loss: 0.0711 - val_loss: 0.1904\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - ETA: 1s - loss: 0.067 - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - 2s 197ms/step - loss: 0.0670 - val_loss: 0.1917\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 2s 219ms/step - loss: 0.0647 - val_loss: 0.1891\n",
            "roc_auc 0.5425346084981877\n",
            "pr_auc_norm where normal is the positive class 0.11293405594719379\n",
            "pr_auc_norm where anomaly is the positive class 0.9113179175001034\n",
            "Class: 8\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 12s - loss: 0.38 - ETA: 0s - loss: 0.3601 - ETA: 1s - loss: 0.338 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.254 - 3s 214ms/step - loss: 0.2543 - val_loss: 0.2364\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.154 - ETA: 1s - loss: 0.154 - ETA: 1s - loss: 0.147 - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.127 - 1s 165ms/step - loss: 0.1281 - val_loss: 0.1965\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.099 - ETA: 1s - loss: 0.099 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.099 - 2s 195ms/step - loss: 0.0990 - val_loss: 0.1781\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.089 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - 2s 188ms/step - loss: 0.0849 - val_loss: 0.1680\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.075 - ETA: 1s - loss: 0.077 - ETA: 1s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - 1s 154ms/step - loss: 0.0760 - val_loss: 0.1672\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 2s 173ms/step - loss: 0.0703 - val_loss: 0.1683\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - 1s 147ms/step - loss: 0.0657 - val_loss: 0.1725\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.061 - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - 2s 173ms/step - loss: 0.0626 - val_loss: 0.1714\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - 1s 159ms/step - loss: 0.0585 - val_loss: 0.1803\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.051 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.055 - 1s 135ms/step - loss: 0.0560 - val_loss: 0.1701\n",
            "roc_auc 0.6971318222109663\n",
            "pr_auc_norm where normal is the positive class 0.2679001795918332\n",
            "pr_auc_norm where anomaly is the positive class 0.9449217166601873\n",
            "Class: 9\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.458 - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.308 - 3s 205ms/step - loss: 0.3087 - val_loss: 0.2763\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.198 - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.171 - 1s 159ms/step - loss: 0.1711 - val_loss: 0.2429\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.129 - 1s 169ms/step - loss: 0.1290 - val_loss: 0.2131\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.115 - ETA: 1s - loss: 0.114 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.111 - 1s 180ms/step - loss: 0.1111 - val_loss: 0.1993\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - 1s 159ms/step - loss: 0.0989 - val_loss: 0.1909\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - 1s 153ms/step - loss: 0.0906 - val_loss: 0.1931\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - 1s 182ms/step - loss: 0.0828 - val_loss: 0.1957\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.080 - ETA: 1s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 2s 200ms/step - loss: 0.0771 - val_loss: 0.1962\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.073 - ETA: 1s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 1s 181ms/step - loss: 0.0718 - val_loss: 0.1976\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.070 - ETA: 1s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 1s 164ms/step - loss: 0.0676 - val_loss: 0.2007\n",
            "roc_auc 0.6083670548502352\n",
            "pr_auc_norm where normal is the positive class 0.13357753652806284\n",
            "pr_auc_norm where anomaly is the positive class 0.9306709226873\n",
            "Run Number: 2\n",
            "Class: 0\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.465 - ETA: 1s - loss: 0.419 - ETA: 0s - loss: 0.381 - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.284 - 3s 220ms/step - loss: 0.2842 - val_loss: 0.2695\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.139 - 1s 178ms/step - loss: 0.1391 - val_loss: 0.2291\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.113 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.102 - 1s 175ms/step - loss: 0.1022 - val_loss: 0.1990\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - 1s 178ms/step - loss: 0.0848 - val_loss: 0.1939\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - 1s 166ms/step - loss: 0.0754 - val_loss: 0.1868\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 1s 179ms/step - loss: 0.0685 - val_loss: 0.1980\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.061 - ETA: 1s - loss: 0.062 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 2s 204ms/step - loss: 0.0636 - val_loss: 0.1976\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 2s 204ms/step - loss: 0.0595 - val_loss: 0.1943\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.054 - 1s 180ms/step - loss: 0.0549 - val_loss: 0.1984\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.055 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.054 - 1s 175ms/step - loss: 0.0547 - val_loss: 0.2022\n",
            "roc_auc 0.6106168349731034\n",
            "pr_auc_norm where normal is the positive class 0.1531796183467482\n",
            "pr_auc_norm where anomaly is the positive class 0.9271078100608956\n",
            "Class: 1\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 8s - loss: 0.445 - ETA: 1s - loss: 0.427 - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.386 - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.334 - ETA: 0s - loss: 0.326 - 3s 203ms/step - loss: 0.3260 - val_loss: 0.2796\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.227 - ETA: 1s - loss: 0.217 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.184 - 1s 181ms/step - loss: 0.1846 - val_loss: 0.2451\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.149 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.138 - 1s 167ms/step - loss: 0.1380 - val_loss: 0.2162\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.120 - ETA: 1s - loss: 0.123 - ETA: 1s - loss: 0.122 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.117 - 2s 211ms/step - loss: 0.1177 - val_loss: 0.2019\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.107 - ETA: 1s - loss: 0.106 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.105 - 1s 163ms/step - loss: 0.1056 - val_loss: 0.2040\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - 2s 198ms/step - loss: 0.0967 - val_loss: 0.2067\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - 1s 171ms/step - loss: 0.0899 - val_loss: 0.2088\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - 1s 164ms/step - loss: 0.0836 - val_loss: 0.2067\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 174ms/step - loss: 0.0786 - val_loss: 0.2060\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 165ms/step - loss: 0.0739 - val_loss: 0.2088\n",
            "roc_auc 0.5098854203924568\n",
            "pr_auc_norm where normal is the positive class 0.09592870460073276\n",
            "pr_auc_norm where anomaly is the positive class 0.9257380183560698\n",
            "Class: 2\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 12s - loss: 0.36 - ETA: 0s - loss: 0.3406 - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.290 - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.237 - ETA: 0s - loss: 0.236 - 3s 194ms/step - loss: 0.2367 - val_loss: 0.1988\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.145 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.120 - ETA: 0s - loss: 0.121 - 1s 138ms/step - loss: 0.1210 - val_loss: 0.1763\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - 1s 149ms/step - loss: 0.0933 - val_loss: 0.1662\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.084 - ETA: 1s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - 2s 201ms/step - loss: 0.0800 - val_loss: 0.1622\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.072 - ETA: 1s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 1s 167ms/step - loss: 0.0731 - val_loss: 0.1597\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 1s 170ms/step - loss: 0.0699 - val_loss: 0.1593\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.067 - ETA: 1s - loss: 0.065 - ETA: 1s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - 2s 169ms/step - loss: 0.0654 - val_loss: 0.1609\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.065 - ETA: 1s - loss: 0.064 - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 1s 157ms/step - loss: 0.0634 - val_loss: 0.1544\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.057 - ETA: 1s - loss: 0.060 - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 2s 197ms/step - loss: 0.0597 - val_loss: 0.1548\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - 2s 182ms/step - loss: 0.0567 - val_loss: 0.1553\n",
            "roc_auc 0.6685888888888888\n",
            "pr_auc_norm where normal is the positive class 0.19389231326007828\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pr_auc_norm where anomaly is the positive class 0.9480628492030736\n",
            "Class: 3\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.434 - ETA: 1s - loss: 0.392 - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.285 - 3s 235ms/step - loss: 0.2855 - val_loss: 0.2434\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.173 - ETA: 1s - loss: 0.175 - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.150 - 1s 187ms/step - loss: 0.1507 - val_loss: 0.2047\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.118 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - 1s 184ms/step - loss: 0.1115 - val_loss: 0.1860\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.096 - ETA: 1s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 2s 208ms/step - loss: 0.0951 - val_loss: 0.1744\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - 2s 229ms/step - loss: 0.0859 - val_loss: 0.1696\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.081 - ETA: 1s - loss: 0.081 - ETA: 1s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - 1s 192ms/step - loss: 0.0783 - val_loss: 0.1747\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.077 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 176ms/step - loss: 0.0729 - val_loss: 0.1733\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.069 - ETA: 1s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 2s 213ms/step - loss: 0.0685 - val_loss: 0.1765\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 1s 164ms/step - loss: 0.0647 - val_loss: 0.1763\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.062 - ETA: 1s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - 1s 184ms/step - loss: 0.0607 - val_loss: 0.1780\n",
            "roc_auc 0.5114567436222143\n",
            "pr_auc_norm where normal is the positive class 0.10048881904592494\n",
            "pr_auc_norm where anomaly is the positive class 0.9050164185848288\n",
            "Class: 4\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.32 - ETA: 0s - loss: 0.2936 - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.219 - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.204 - 3s 213ms/step - loss: 0.2044 - val_loss: 0.2014\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.122 - ETA: 1s - loss: 0.120 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.114 - 1s 190ms/step - loss: 0.1148 - val_loss: 0.1844\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - 1s 142ms/step - loss: 0.0863 - val_loss: 0.1754\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 145ms/step - loss: 0.0722 - val_loss: 0.1627\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.059 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 1s 192ms/step - loss: 0.0645 - val_loss: 0.1623\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 1s 198ms/step - loss: 0.0590 - val_loss: 0.1598\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.056 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.055 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - 2s 203ms/step - loss: 0.0539 - val_loss: 0.1614\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.050 - ETA: 1s - loss: 0.050 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - 1s 178ms/step - loss: 0.0504 - val_loss: 0.1637\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.051 - ETA: 1s - loss: 0.049 - ETA: 0s - loss: 0.049 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - 1s 188ms/step - loss: 0.0478 - val_loss: 0.1612\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.045 - 1s 187ms/step - loss: 0.0452 - val_loss: 0.1622\n",
            "roc_auc 0.6898046398046397\n",
            "pr_auc_norm where normal is the positive class 0.18287450700137\n",
            "pr_auc_norm where anomaly is the positive class 0.9515177460777586\n",
            "Class: 5\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.413 - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.361 - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.280 - 3s 196ms/step - loss: 0.2809 - val_loss: 0.2469\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.152 - 1s 182ms/step - loss: 0.1528 - val_loss: 0.2236\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.114 - 1s 173ms/step - loss: 0.1144 - val_loss: 0.2051\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.103 - ETA: 1s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.097 - 1s 171ms/step - loss: 0.0977 - val_loss: 0.1969\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - 1s 173ms/step - loss: 0.0887 - val_loss: 0.1874\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.082 - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - 1s 176ms/step - loss: 0.0824 - val_loss: 0.1911\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - ETA: 1s - loss: 0.077 - ETA: 1s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - 1s 189ms/step - loss: 0.0757 - val_loss: 0.1875\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 172ms/step - loss: 0.0729 - val_loss: 0.1872\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 1s 178ms/step - loss: 0.0683 - val_loss: 0.1912\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 1s 157ms/step - loss: 0.0643 - val_loss: 0.1921\n",
            "roc_auc 0.487227113124014\n",
            "pr_auc_norm where normal is the positive class 0.0798646762244975\n",
            "pr_auc_norm where anomaly is the positive class 0.9159604316704364\n",
            "Class: 6\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 10s - loss: 0.33 - ETA: 0s - loss: 0.3254 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.226 - 3s 225ms/step - loss: 0.2256 - val_loss: 0.1985\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.136 - ETA: 1s - loss: 0.127 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.124 - 1s 170ms/step - loss: 0.1248 - val_loss: 0.1592\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.107 - ETA: 1s - loss: 0.105 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.099 - 2s 184ms/step - loss: 0.0990 - val_loss: 0.1637\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.084 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - 2s 180ms/step - loss: 0.0854 - val_loss: 0.1582\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.085 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 2s 201ms/step - loss: 0.0781 - val_loss: 0.1519\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 2s 189ms/step - loss: 0.0728 - val_loss: 0.1522\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 2s 173ms/step - loss: 0.0688 - val_loss: 0.1451\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - 2s 172ms/step - loss: 0.0656 - val_loss: 0.1479\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.064 - ETA: 1s - loss: 0.061 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - 1s 140ms/step - loss: 0.0626 - val_loss: 0.1469\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.059 - ETA: 1s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - 1s 165ms/step - loss: 0.0613 - val_loss: 0.1462\n",
            "roc_auc 0.8056024774774776\n",
            "pr_auc_norm where normal is the positive class 0.34821809151823757\n",
            "pr_auc_norm where anomaly is the positive class 0.9687061774467735\n",
            "Class: 7\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 11s - loss: 0.43 - ETA: 0s - loss: 0.4108 - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.358 - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.292 - 3s 200ms/step - loss: 0.2925 - val_loss: 0.2368\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.154 - 1s 164ms/step - loss: 0.1545 - val_loss: 0.2061\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.124 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.116 - 1s 160ms/step - loss: 0.1168 - val_loss: 0.1904\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - 1s 178ms/step - loss: 0.0988 - val_loss: 0.1849\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - 1s 173ms/step - loss: 0.0882 - val_loss: 0.1761\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - 1s 170ms/step - loss: 0.0814 - val_loss: 0.1734\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.073 - ETA: 1s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - 1s 182ms/step - loss: 0.0759 - val_loss: 0.1791\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 159ms/step - loss: 0.0717 - val_loss: 0.1741\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 1s 172ms/step - loss: 0.0697 - val_loss: 0.1772\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - 1s 170ms/step - loss: 0.0654 - val_loss: 0.1743\n",
            "roc_auc 0.555253504519848\n",
            "pr_auc_norm where normal is the positive class 0.11746872427759315\n",
            "pr_auc_norm where anomaly is the positive class 0.9138413801221863\n",
            "Class: 8\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 11s - loss: 0.42 - ETA: 1s - loss: 0.3965 - ETA: 1s - loss: 0.365 - ETA: 0s - loss: 0.342 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.288 - ETA: 0s - loss: 0.274 - 3s 190ms/step - loss: 0.2743 - val_loss: 0.2438\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.158 - ETA: 1s - loss: 0.155 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.136 - 1s 149ms/step - loss: 0.1368 - val_loss: 0.2070\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.104 - ETA: 1s - loss: 0.105 - ETA: 1s - loss: 0.106 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - 1s 157ms/step - loss: 0.0993 - val_loss: 0.1817\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.083 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - 1s 145ms/step - loss: 0.0852 - val_loss: 0.1739\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.085 - ETA: 1s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - 1s 167ms/step - loss: 0.0761 - val_loss: 0.1636\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.077 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 1s 157ms/step - loss: 0.0694 - val_loss: 0.1719\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.064 - 1s 147ms/step - loss: 0.0646 - val_loss: 0.1723\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - 1s 151ms/step - loss: 0.0609 - val_loss: 0.1767\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.060 - ETA: 1s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - 1s 149ms/step - loss: 0.0626 - val_loss: 0.1853\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.061 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 1s 145ms/step - loss: 0.0599 - val_loss: 0.1811\n",
            "roc_auc 0.678168924908193\n",
            "pr_auc_norm where normal is the positive class 0.25754976394536166\n",
            "pr_auc_norm where anomaly is the positive class 0.9411976998596276\n",
            "Class: 9\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 11s - loss: 0.44 - ETA: 1s - loss: 0.4089 - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.374 - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.315 - 3s 217ms/step - loss: 0.3152 - val_loss: 0.2749\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.197 - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.173 - 1s 176ms/step - loss: 0.1732 - val_loss: 0.2384\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - 1s 149ms/step - loss: 0.1301 - val_loss: 0.2077\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.110 - 2s 205ms/step - loss: 0.1108 - val_loss: 0.1936\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.098 - 1s 171ms/step - loss: 0.0985 - val_loss: 0.1913\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.090 - 1s 159ms/step - loss: 0.0903 - val_loss: 0.1889\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.085 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - 1s 159ms/step - loss: 0.0822 - val_loss: 0.1841\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - 1s 172ms/step - loss: 0.0769 - val_loss: 0.1893\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 156ms/step - loss: 0.0724 - val_loss: 0.1839\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 1s 169ms/step - loss: 0.0686 - val_loss: 0.1858\n",
            "roc_auc 0.5909657224641934\n",
            "pr_auc_norm where normal is the positive class 0.12712867175845233\n",
            "pr_auc_norm where anomaly is the positive class 0.9256640930503202\n",
            "Run Number: 3\n",
            "Class: 0\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.44 - ETA: 0s - loss: 0.4010 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.280 - 3s 220ms/step - loss: 0.2807 - val_loss: 0.2769\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.144 - 1s 186ms/step - loss: 0.1445 - val_loss: 0.2412\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.108 - ETA: 1s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.106 - 1s 177ms/step - loss: 0.1066 - val_loss: 0.2062\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.096 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - 1s 170ms/step - loss: 0.0890 - val_loss: 0.1904\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.080 - ETA: 1s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 176ms/step - loss: 0.0778 - val_loss: 0.1842\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 1s 167ms/step - loss: 0.0705 - val_loss: 0.1833\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.067 - ETA: 1s - loss: 0.067 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - 1s 180ms/step - loss: 0.0658 - val_loss: 0.1811\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.062 - 1s 161ms/step - loss: 0.0621 - val_loss: 0.1865\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 1s 166ms/step - loss: 0.0592 - val_loss: 0.1832\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - 1s 157ms/step - loss: 0.0549 - val_loss: 0.1907\n",
            "roc_auc 0.5767444880994902\n",
            "pr_auc_norm where normal is the positive class 0.1322701862567473\n",
            "pr_auc_norm where anomaly is the positive class 0.9187449734766535\n",
            "Class: 1\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.452 - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.370 - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.310 - 3s 220ms/step - loss: 0.3101 - val_loss: 0.2820\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.211 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.191 - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.185 - 1s 178ms/step - loss: 0.1854 - val_loss: 0.2529\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.138 - 1s 145ms/step - loss: 0.1383 - val_loss: 0.2265\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.116 - ETA: 1s - loss: 0.116 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.113 - 1s 143ms/step - loss: 0.1139 - val_loss: 0.2055\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.103 - 1s 183ms/step - loss: 0.1030 - val_loss: 0.2011\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - 1s 172ms/step - loss: 0.0930 - val_loss: 0.1947\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.091 - ETA: 1s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.087 - 1s 171ms/step - loss: 0.0870 - val_loss: 0.1995\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - 1s 184ms/step - loss: 0.0811 - val_loss: 0.1950\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.079 - ETA: 1s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - 1s 168ms/step - loss: 0.0765 - val_loss: 0.1975\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 1s 153ms/step - loss: 0.0722 - val_loss: 0.2004\n",
            "roc_auc 0.5123768176716536\n",
            "pr_auc_norm where normal is the positive class 0.09645143211195896\n",
            "pr_auc_norm where anomaly is the positive class 0.9251827316408834\n",
            "Class: 2\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 11s - loss: 0.36 - ETA: 1s - loss: 0.3343 - ETA: 1s - loss: 0.304 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.225 - 3s 194ms/step - loss: 0.2245 - val_loss: 0.1970\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.123 - ETA: 1s - loss: 0.121 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.113 - 1s 147ms/step - loss: 0.1132 - val_loss: 0.1788\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.099 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.093 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.090 - 1s 169ms/step - loss: 0.0900 - val_loss: 0.1750\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 141ms/step - loss: 0.0797 - val_loss: 0.1697\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 166ms/step - loss: 0.0729 - val_loss: 0.1669\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 1s 164ms/step - loss: 0.0676 - val_loss: 0.1649\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.062 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 1s 155ms/step - loss: 0.0634 - val_loss: 0.1603\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.061 - ETA: 1s - loss: 0.062 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - 1s 144ms/step - loss: 0.0605 - val_loss: 0.1576\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 1s 159ms/step - loss: 0.0575 - val_loss: 0.1554\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.054 - 1s 153ms/step - loss: 0.0549 - val_loss: 0.1597\n",
            "roc_auc 0.6657833333333334\n",
            "pr_auc_norm where normal is the positive class 0.1924877830413389\n",
            "pr_auc_norm where anomaly is the positive class 0.9464876550334566\n",
            "Class: 3\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.40 - ETA: 1s - loss: 0.3652 - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.278 - ETA: 0s - loss: 0.266 - 3s 219ms/step - loss: 0.2662 - val_loss: 0.2426\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.159 - ETA: 1s - loss: 0.166 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.142 - 1s 182ms/step - loss: 0.1429 - val_loss: 0.2111\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.112 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.107 - 1s 181ms/step - loss: 0.1079 - val_loss: 0.1890\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - 1s 173ms/step - loss: 0.0919 - val_loss: 0.1806\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - 1s 142ms/step - loss: 0.0818 - val_loss: 0.1797\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - 1s 181ms/step - loss: 0.0746 - val_loss: 0.1845\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 1s 173ms/step - loss: 0.0684 - val_loss: 0.1868\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.067 - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 1s 160ms/step - loss: 0.0634 - val_loss: 0.1886\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - 1s 174ms/step - loss: 0.0610 - val_loss: 0.1864\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - 1s 188ms/step - loss: 0.0573 - val_loss: 0.1825\n",
            "roc_auc 0.5209057159247112\n",
            "pr_auc_norm where normal is the positive class 0.10216397489788533\n",
            "pr_auc_norm where anomaly is the positive class 0.9055753786702956\n",
            "Class: 4\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.36 - ETA: 1s - loss: 0.3337 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.263 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.240 - 3s 213ms/step - loss: 0.2404 - val_loss: 0.2040\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.152 - ETA: 1s - loss: 0.143 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.124 - 1s 184ms/step - loss: 0.1246 - val_loss: 0.1783\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.095 - ETA: 1s - loss: 0.092 - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - 1s 177ms/step - loss: 0.0913 - val_loss: 0.1654\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 174ms/step - loss: 0.0776 - val_loss: 0.1656\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.075 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - 1s 179ms/step - loss: 0.0679 - val_loss: 0.1686\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - 1s 154ms/step - loss: 0.0612 - val_loss: 0.1681\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.061 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - 1s 179ms/step - loss: 0.0568 - val_loss: 0.1676\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.050 - ETA: 1s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - ETA: 0s - loss: 0.053 - 1s 189ms/step - loss: 0.0535 - val_loss: 0.1643\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.051 - ETA: 1s - loss: 0.051 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - 1s 154ms/step - loss: 0.0503 - val_loss: 0.1627\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.048 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.047 - 1s 175ms/step - loss: 0.0472 - val_loss: 0.1630\n",
            "roc_auc 0.6794993894993895\n",
            "pr_auc_norm where normal is the positive class 0.1659826825831428\n",
            "pr_auc_norm where anomaly is the positive class 0.9510367070063962\n",
            "Class: 5\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.38 - ETA: 1s - loss: 0.3721 - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.262 - 3s 194ms/step - loss: 0.2629 - val_loss: 0.2451\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.154 - ETA: 1s - loss: 0.154 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.138 - 1s 179ms/step - loss: 0.1388 - val_loss: 0.2189\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - 1s 160ms/step - loss: 0.1073 - val_loss: 0.2006\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - 1s 164ms/step - loss: 0.0931 - val_loss: 0.1847\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.087 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - 1s 157ms/step - loss: 0.0847 - val_loss: 0.1880\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 165ms/step - loss: 0.0782 - val_loss: 0.1862\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 163ms/step - loss: 0.0733 - val_loss: 0.1858\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 1s 134ms/step - loss: 0.0691 - val_loss: 0.1892\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - 1s 154ms/step - loss: 0.0646 - val_loss: 0.1892\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.061 - 1s 180ms/step - loss: 0.0610 - val_loss: 0.1911\n",
            "roc_auc 0.4927484606381355\n",
            "pr_auc_norm where normal is the positive class 0.08004512985323908\n",
            "pr_auc_norm where anomaly is the positive class 0.917667982876641\n",
            "Class: 6\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - ETA: 16s - loss: 0.40 - ETA: 1s - loss: 0.3784 - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.254 - 4s 196ms/step - loss: 0.2543 - val_loss: 0.2127\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.153 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.129 - 1s 153ms/step - loss: 0.1291 - val_loss: 0.1858\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.104 - ETA: 1s - loss: 0.104 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.098 - 1s 147ms/step - loss: 0.0982 - val_loss: 0.1728\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.089 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - 1s 149ms/step - loss: 0.0862 - val_loss: 0.1659\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.078 - ETA: 1s - loss: 0.078 - ETA: 1s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - 1s 155ms/step - loss: 0.0801 - val_loss: 0.1591\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.077 - ETA: 1s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - 1s 148ms/step - loss: 0.0771 - val_loss: 0.1544\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - 1s 155ms/step - loss: 0.0745 - val_loss: 0.1486\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 1s 141ms/step - loss: 0.0702 - val_loss: 0.1460\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.068 - ETA: 1s - loss: 0.068 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.065 - 1s 169ms/step - loss: 0.0656 - val_loss: 0.1490\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.065 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.063 - 1s 141ms/step - loss: 0.0634 - val_loss: 0.1431\n",
            "roc_auc 0.7956382722007722\n",
            "pr_auc_norm where normal is the positive class 0.3198779376113573\n",
            "pr_auc_norm where anomaly is the positive class 0.9667515676221334\n",
            "Class: 7\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 9s - loss: 0.404 - ETA: 1s - loss: 0.377 - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.335 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.278 - 3s 210ms/step - loss: 0.2781 - val_loss: 0.2371\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.173 - ETA: 0s - loss: 0.171 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.155 - 1s 163ms/step - loss: 0.1559 - val_loss: 0.2135\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.126 - ETA: 0s - loss: 0.128 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.121 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.115 - 1s 185ms/step - loss: 0.1158 - val_loss: 0.1985\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.110 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.097 - 1s 176ms/step - loss: 0.0978 - val_loss: 0.1894\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - ETA: 0s - loss: 0.088 - 1s 181ms/step - loss: 0.0880 - val_loss: 0.1856\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.080 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - 1s 185ms/step - loss: 0.0813 - val_loss: 0.1837\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.078 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.075 - 1s 183ms/step - loss: 0.0759 - val_loss: 0.1852\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 1s 169ms/step - loss: 0.0707 - val_loss: 0.1885\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.066 - ETA: 1s - loss: 0.065 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 2s 198ms/step - loss: 0.0672 - val_loss: 0.1874\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - 1s 168ms/step - loss: 0.0631 - val_loss: 0.1867\n",
            "roc_auc 0.5637691602253374\n",
            "pr_auc_norm where normal is the positive class 0.12942585321131345\n",
            "pr_auc_norm where anomaly is the positive class 0.9103458116752263\n",
            "Class: 8\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 11s - loss: 0.41 - ETA: 1s - loss: 0.3815 - ETA: 0s - loss: 0.351 - ETA: 0s - loss: 0.331 - ETA: 0s - loss: 0.309 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.270 - 3s 196ms/step - loss: 0.2703 - val_loss: 0.2449\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.173 - ETA: 1s - loss: 0.168 - ETA: 1s - loss: 0.162 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.143 - 2s 170ms/step - loss: 0.1432 - val_loss: 0.2136\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.115 - ETA: 1s - loss: 0.114 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - ETA: 0s - loss: 0.111 - ETA: 0s - loss: 0.110 - ETA: 0s - loss: 0.109 - ETA: 0s - loss: 0.108 - 1s 151ms/step - loss: 0.1080 - val_loss: 0.1900\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.092 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.089 - 2s 184ms/step - loss: 0.0895 - val_loss: 0.1871\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.080 - ETA: 1s - loss: 0.080 - ETA: 1s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 160ms/step - loss: 0.0782 - val_loss: 0.1885\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.074 - ETA: 1s - loss: 0.073 - ETA: 1s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 1s 164ms/step - loss: 0.0716 - val_loss: 0.1904\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - ETA: 1s - loss: 0.064 - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 1s 158ms/step - loss: 0.0690 - val_loss: 0.1912\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.064 - ETA: 1s - loss: 0.066 - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - 1s 165ms/step - loss: 0.0628 - val_loss: 0.1875\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 1s - loss: 0.060 - ETA: 1s - loss: 0.061 - ETA: 1s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.058 - 2s 179ms/step - loss: 0.0587 - val_loss: 0.1832\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - 1s 115ms/step - loss: 0.0546 - val_loss: 0.1777\n",
            "roc_auc 0.6745071968257988\n",
            "pr_auc_norm where normal is the positive class 0.2229242965978207\n",
            "pr_auc_norm where anomaly is the positive class 0.9403880663354116\n",
            "Class: 9\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 10s - loss: 0.41 - ETA: 0s - loss: 0.4061 - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.333 - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.311 - 3s 227ms/step - loss: 0.3117 - val_loss: 0.2748\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.212 - ETA: 1s - loss: 0.205 - ETA: 0s - loss: 0.196 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.174 - 1s 173ms/step - loss: 0.1740 - val_loss: 0.2345\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.135 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.129 - 1s 181ms/step - loss: 0.1298 - val_loss: 0.2034\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - ETA: 1s - loss: 0.116 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.114 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.112 - 1s 181ms/step - loss: 0.1125 - val_loss: 0.1878\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.103 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.102 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.101 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.100 - 1s 159ms/step - loss: 0.1007 - val_loss: 0.1814\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.091 - 1s 179ms/step - loss: 0.0913 - val_loss: 0.1813\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - 1s 147ms/step - loss: 0.0842 - val_loss: 0.1803\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 176ms/step - loss: 0.0782 - val_loss: 0.1814\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.075 - ETA: 1s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 171ms/step - loss: 0.0734 - val_loss: 0.1804\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 1s 155ms/step - loss: 0.0690 - val_loss: 0.1880\n",
            "roc_auc 0.6197757390417941\n",
            "pr_auc_norm where normal is the positive class 0.1350782895967106\n",
            "pr_auc_norm where anomaly is the positive class 0.9353447094964682\n",
            "END _transformations_experiment \n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    N_GPUS = 1\n",
        "    man = Manager()\n",
        "    q = man.Queue(N_GPUS)\n",
        "    for g in range(N_GPUS):\n",
        "        q.put(str(g))\n",
        "\n",
        "    experiments_list = [ \n",
        "        (load_cifar10, 'cifar10', 10),\n",
        "    ]\n",
        "\n",
        "    for data_load_fn, dataset_name, n_classes in experiments_list:\n",
        "        run_experiments(data_load_fn, dataset_name, q, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xcuNgKkDM9I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "gpu2",
      "language": "python",
      "name": "gpu2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}